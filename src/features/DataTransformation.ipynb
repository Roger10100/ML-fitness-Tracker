{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9363c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-utils in /home/ojas-srivastava/anaconda3/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>3.10.0.2 in /home/ojas-srivastava/anaconda3/lib/python3.11/site-packages (from python-utils) (4.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210ae89",
   "metadata": {
    "cell_marker": "##############################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "                                                           #\n",
    "   Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "   Machine Learning for the Quantified Self                #\n",
    "   Springer                                                #\n",
    "   Chapter 3                                               #\n",
    "                                                           #\n",
    "#############################################################\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad4f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "import pandas as pd\n",
    "\n",
    "# This class removes the high frequency data (that might be considered noise) from the data.\n",
    "class LowPassFilter:\n",
    "\n",
    "    def low_pass_filter(self, data_table, col, sampling_frequency, cutoff_frequency, order=5, phase_shift=True):\n",
    "        # Cutoff frequencies are expressed as the fraction of the Nyquist frequency, which is half the sampling frequency\n",
    "        nyq = 0.5 * sampling_frequency\n",
    "        cut = cutoff_frequency / nyq\n",
    "\n",
    "        b, a = butter(order, cut, btype='low', output='ba', analog=False)\n",
    "        if phase_shift:\n",
    "            data_table[col + '_lowpass'] = filtfilt(b, a, data_table[col])\n",
    "        else:\n",
    "            data_table[col + '_lowpass'] = lfilter(b, a, data_table[col])\n",
    "        return data_table\n",
    "\n",
    "\n",
    "# Class for Principal Component Analysis. We can only apply this when we do not have missing values (i.e. NaN).\n",
    "class PrincipalComponentAnalysis:\n",
    "\n",
    "    pca = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pca = []\n",
    "\n",
    "    # Normalize the dataset using StandardScaler from sklearn\n",
    "    def normalize_dataset(self, data_table, cols):\n",
    "        scaler = StandardScaler()\n",
    "        # Fit the scaler on the selected columns and transform them\n",
    "        scaled_values = scaler.fit_transform(data_table[cols])\n",
    "        # Convert the scaled values back into a DataFrame to maintain column structure\n",
    "        data_table[cols] = pd.DataFrame(scaled_values, columns=cols, index=data_table.index)\n",
    "        return data_table\n",
    "\n",
    "    # Perform the PCA on the selected columns and return the explained variance.\n",
    "    def determine_pc_explained_variance(self, data_table, cols):\n",
    "        # Normalize the data first.\n",
    "        data_table = self.normalize_dataset(data_table, cols)\n",
    "\n",
    "        # Perform PCA.\n",
    "        self.pca = PCA(n_components=len(cols))\n",
    "        self.pca.fit(data_table[cols])\n",
    "        # Return the explained variances.\n",
    "        return self.pca.explained_variance_ratio_\n",
    "\n",
    "    # Apply PCA and add new PCA columns to the dataset.\n",
    "    def apply_pca(self, data_table, cols, number_comp):\n",
    "        # Normalize the data first.\n",
    "        data_table = self.normalize_dataset(data_table, cols)\n",
    "\n",
    "        # Perform PCA.\n",
    "        self.pca = PCA(n_components=number_comp)\n",
    "        self.pca.fit(data_table[cols])\n",
    "\n",
    "        # Transform the old values.\n",
    "        new_values = self.pca.transform(data_table[cols])\n",
    "\n",
    "        # Add the new PCA columns.\n",
    "        for comp in range(0, number_comp):\n",
    "            data_table['pca_' + str(comp + 1)] = new_values[:, comp]\n",
    "\n",
    "        return data_table\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
