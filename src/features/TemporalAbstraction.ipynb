{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bff1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ec625",
   "metadata": {
    "cell_marker": "##############################################################"
   },
   "source": [
    "                                                           #\n",
    "   Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "   Machine Learning for the Quantified Self                #\n",
    "   Springer                                                #\n",
    "   Chapter 4                                               #\n",
    "                                                           #\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729e1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Class to abstract a history of numerical values we can use as an attribute.\n",
    "class NumericalAbstraction:\n",
    "\n",
    "    # For the slope we need a bit more work.\n",
    "    # We create time points, assuming discrete time steps with fixed delta t:\n",
    "    def get_slope(self, data):\n",
    "        \n",
    "        times = np.array(range(0, len(data.index)))\n",
    "        data = data.astype(np.float32)\n",
    "\n",
    "        # Check for NaN's\n",
    "        mask = ~np.isnan(data)\n",
    "\n",
    "        # If we have no data but NaN we return NaN.\n",
    "        if (len(data[mask]) == 0):\n",
    "            return np.nan\n",
    "        # Otherwise we return the slope.\n",
    "        else:\n",
    "            slope, _, _, _, _ = stats.linregress(times[mask], data[mask])\n",
    "            return slope\n",
    "\n",
    "    #TODO Add your own aggregation function here:\n",
    "    # def my_aggregation_function(self, data) \n",
    "\n",
    "    # This function aggregates a list of values using the specified aggregation\n",
    "    # function (which can be 'mean', 'max', 'min', 'median', 'std', 'slope')\n",
    "    def aggregate_value(self,data, window_size, aggregation_function):\n",
    "        window = str(window_size) + 's'\n",
    "        # Compute the values and return the result.\n",
    "        if aggregation_function == 'mean':\n",
    "            return data.rolling(window, min_periods=window_size).mean()\n",
    "        elif aggregation_function == 'max':\n",
    "            return data.rolling(window, min_periods=window_size).max()\n",
    "        elif aggregation_function == 'min':\n",
    "            return data.rolling(window, min_periods=window_size).min()\n",
    "        elif aggregation_function == 'median':\n",
    "            return data.rolling(window, min_periods=window_size).median()\n",
    "        elif aggregation_function == 'std':\n",
    "            return data.rolling(window, min_periods=window_size).std()\n",
    "        elif aggregation_function == 'slope':\n",
    "            return data.rolling(window, min_periods=window_size).apply(self.get_slope)\n",
    "        \n",
    "        #TODO: add your own aggregation function here\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "    def abstract_numerical(self, data_table, cols, window_size, aggregation_function_name):\n",
    "    \n",
    "        for col in cols:\n",
    "            \n",
    "            aggregations = self.aggregate_value(data_table[col], window_size, aggregation_function_name)\n",
    "            data_table[col + '_temp_' + aggregation_function_name + '_ws_' + str(window_size)] = aggregations\n",
    "      \n",
    "        \n",
    "        return data_table\n",
    "\n",
    "# Class to perform categorical abstraction. We obtain patterns of categorical attributes that occur frequently\n",
    "# over time.\n",
    "class CategoricalAbstraction:\n",
    "\n",
    "    pattern_prefix = 'temp_pattern_'\n",
    "    before = '(b)'\n",
    "    co_occurs = '(c)'\n",
    "    cache = {}\n",
    "\n",
    "    # Determine the time points a pattern occurs in the dataset given a windows size.\n",
    "    def determine_pattern_times(self, data_table, pattern, window_size):\n",
    "        times = []\n",
    "\n",
    "        # If we have a pattern of length one\n",
    "        if len(pattern) == 1:\n",
    "            # If it is in the cache, we get the times from the cache.\n",
    "            if self.to_string(pattern) in self.cache:\n",
    "                times = self.cache[self.to_string(pattern)]\n",
    "            # Otherwise we identify the time points at which we observe the value.\n",
    "            else:\n",
    "               \n",
    "                timestamp_rows = data_table[data_table[pattern[0]] > 0].index.values.tolist()\n",
    "               \n",
    "                times = [data_table.index.get_loc(i) for i in timestamp_rows]\n",
    "                self.cache[self.to_string(pattern)] = times\n",
    "\n",
    "        # If we have a complex pattern (<n> (b) <m> or <n> (c) <m>)\n",
    "        elif len(pattern) == 3:\n",
    "            # We computer the time points of <n> and <m>\n",
    "            time_points_first_part = self.determine_pattern_times(data_table, pattern[0], window_size)\n",
    "            time_points_second_part = self.determine_pattern_times(data_table, pattern[2], window_size)\n",
    "\n",
    "            # If it co-occurs we take the intersection.\n",
    "            if pattern[1] == self.co_occurs:\n",
    "                # No use for co-occurences of the same patterns...\n",
    "                if pattern[0] == pattern[2]:\n",
    "                    times = []\n",
    "                else:\n",
    "                    times = list(set(time_points_first_part) & set(time_points_second_part))\n",
    "            # Otherwise we take all time points from <m> at which we observed <n> within the given\n",
    "            # window size.\n",
    "            elif pattern[1] == self.before:\n",
    "                for t in time_points_second_part:\n",
    "                    if len([i for i in time_points_first_part if ((i >= t - window_size) & (i < t))]):\n",
    "                        times.append(t)\n",
    "        return times\n",
    "\n",
    "    # Create a string representation of a pattern.\n",
    "    def to_string(self, pattern):\n",
    "        # If we just have one component, return the string.\n",
    "        if len(pattern) == 1:\n",
    "            return str(pattern[0])\n",
    "        # Otherwise, return the merger of the strings of all\n",
    "        # components.\n",
    "        else:\n",
    "            name = ''\n",
    "            for p in pattern:\n",
    "                name = name + self.to_string(p)\n",
    "            return name\n",
    "\n",
    "    # Selects the patterns from 'patterns' that meet the minimum support in the dataset\n",
    "    # given the window size.\n",
    "    def select_k_patterns(self, data_table, patterns, min_support, window_size):\n",
    "        selected_patterns = []\n",
    "        for pattern in patterns:\n",
    "            # Determine the times at which the pattern occurs.\n",
    "            times = self.determine_pattern_times(data_table, pattern, window_size)\n",
    "            # Compute the support\n",
    "            support = float(len(times))/len(data_table.index)\n",
    "            # If we meet the minimum support, append the selected patterns and set the\n",
    "            # value to 1 at which it occurs.\n",
    "            if support >= min_support:\n",
    "                selected_patterns.append(pattern)\n",
    "                print(self.to_string(pattern))\n",
    "                # Set the occurrence of the pattern in the row to 0.\n",
    "                data_table[self.pattern_prefix + self.to_string(pattern)] = 0\n",
    "                #data_table[self.pattern_prefix + self.to_string(pattern)][times] = 1\n",
    "                data_table.iloc[times, data_table.columns.get_loc(self.pattern_prefix + self.to_string(pattern))] = 1\n",
    "        return data_table, selected_patterns\n",
    "\n",
    "\n",
    "    # extends a set of k-patterns with the 1-patterns that have sufficient support.\n",
    "    def extend_k_patterns(self, k_patterns, one_patterns):\n",
    "        new_patterns = []\n",
    "        for k_p in k_patterns:\n",
    "            for one_p in one_patterns:\n",
    "                # Add a before relationship\n",
    "                new_patterns.append([k_p, self.before, one_p])\n",
    "                # Add a co-occurs relationship.\n",
    "                new_patterns.append([k_p, self.co_occurs, one_p])\n",
    "        return new_patterns\n",
    "\n",
    "\n",
    "    # Function to abstract our categorical data. Note that we assume a list of binary columns representing\n",
    "    # the different categories. We set whether the column names should match exactly 'exact' or should include the\n",
    "    # specified name 'like'. We also express a minimum support,a windows size between succeeding patterns and a\n",
    "    # maximum size for the number of patterns.\n",
    "    def abstract_categorical(self, data_table, cols, match, min_support, window_size, max_pattern_size):\n",
    "\n",
    "        # Find all the relevant columns of binary attributes.\n",
    "        col_names = list(data_table.columns)\n",
    "        selected_patterns = []\n",
    "\n",
    "        relevant_dataset_cols = []\n",
    "        for i in range(0, len(cols)):\n",
    "            if match[i] == 'exact':\n",
    "                relevant_dataset_cols.append(cols[i])\n",
    "            else:\n",
    "                relevant_dataset_cols.extend([name for name in col_names if cols[i] in name])\n",
    "\n",
    "        # Generate the one patterns first\n",
    "        potential_1_patterns = [[pattern] for pattern in relevant_dataset_cols]\n",
    "\n",
    "        new_data_table, one_patterns = self.select_k_patterns(data_table, potential_1_patterns, min_support, window_size)\n",
    "        selected_patterns.extend(one_patterns)\n",
    "        print(f'Number of patterns of size 1 is {len(one_patterns)}')\n",
    "\n",
    "        k = 1\n",
    "        k_patterns = one_patterns\n",
    "\n",
    "        # And generate all following patterns.\n",
    "        while (k < max_pattern_size) & (len(k_patterns) > 0):\n",
    "            k = k + 1\n",
    "            potential_k_patterns = self.extend_k_patterns(k_patterns, one_patterns)\n",
    "            new_data_table, selected_new_k_patterns = self.select_k_patterns(new_data_table, potential_k_patterns, min_support, window_size)\n",
    "            selected_patterns.extend(selected_new_k_patterns)\n",
    "            print(f'Number of patterns of size {k} is {len(selected_new_k_patterns)}')\n",
    "\n",
    "        return new_data_table\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
